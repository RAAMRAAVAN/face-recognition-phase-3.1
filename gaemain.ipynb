{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [],
   "source": [
    "from __future__ import division\n",
    "from __future__ import print_function\n",
    "\n",
    "import argparse\n",
    "import time\n",
    "\n",
    "import numpy as np\n",
    "import scipy.sparse as sp\n",
    "import torch\n",
    "import torchvision\n",
    "from torch import optim\n",
    "import pandas as pd\n",
    "import os\n",
    "\n",
    "from gae.model import GCNModelVAE\n",
    "\n",
    "from gae.optimizer import loss_function\n",
    "from gae.utils import load_data, mask_test_edges, preprocess_graph, get_roc_score\n",
    "\n",
    "import import_ipynb\n",
    "from ram_mediapipe import mediapipe"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<!-- GAE : Ram -->"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [],
   "source": [
    "parser = argparse.ArgumentParser()\n",
    "parser.add_argument('--model', type=str, default='gcn_vae', help=\"models used\")\n",
    "parser.add_argument('--seed', type=int, default=42, help='Random seed.')\n",
    "parser.add_argument('--epochs', type=int, default=200, help='Number of epochs to train.')\n",
    "parser.add_argument('--hidden1', type=int, default=32, help='Number of units in hidden layer 1.')\n",
    "parser.add_argument('--hidden2', type=int, default=16, help='Number of units in hidden layer 2.')\n",
    "parser.add_argument('--lr', type=float, default=0.01, help='Initial learning rate.')\n",
    "parser.add_argument('--dropout', type=float, default=0., help='Dropout rate (1 - keep probability).')\n",
    "parser.add_argument('--dataset-str', default='cora', help='type of dataset.')\n",
    "\n",
    "args, unknown = parser.parse_known_args()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "metadata": {},
   "outputs": [],
   "source": [
    "def gae_for(path):\n",
    "    # Calling mediapipe for adjacency matrix and feature matrix\n",
    "    # adj: Type=scipy.sparse._csr.csr_matrix, Shape=(468,468)\n",
    "    # Features: Type=torch.Tensor, Shape=(468,468)\n",
    "    adj,features=mediapipe(path)\n",
    "    # n_nodes= 468, feat_dim=468\n",
    "    n_nodes, feat_dim = features.shape \n",
    "    adj_orig = adj\n",
    "    # adj_orig.diagonal() = all diagonal elements. Eg: [0,0,0,.....0], Shape = (1,468), 1 Dimensional\n",
    "    # adj_orig.diagonal()[np.newaxis, :] = [[0,0,0,....,0]], Shape = (1,468), 2 Dimensional\n",
    "    # sp.dia_matrix = sparse matrix with diagonal storage\n",
    "    # sp.dia_matrix((data,offsets),shape) : data = [[0,0,0,...,0]], offsets = [0], and shape = (468,468)\n",
    "    # subtractiong adj matrix with 468*468 matrix of zeros. ( Remove diagonal elements )\n",
    "    adj_orig = adj_orig - sp.dia_matrix((adj_orig.diagonal()[np.newaxis, :], [0]), shape=adj_orig.shape) \n",
    "    # eliminating zeros\n",
    "    adj_orig.eliminate_zeros()\n",
    "    # calling mask_test_edges() function, present in utils\n",
    "    adj_train, train_edges, val_edges, val_edges_false, test_edges, test_edges_false = mask_test_edges(adj) \n",
    "    # 198 edges and lower triangular values are removed from original matrix. \n",
    "    # adj = adj_train\n",
    "    adj_train=adj\n",
    "    # print(\"val_edges_false=\",len(test_edges_false))\n",
    "    # print(train_edges.shape,test_edges,test_edges_false)\n",
    "    # normalize adj matrix : Tilda A = (adj + feature) * pow(D,.5) * pow(D,.5)\n",
    "    adj_norm = preprocess_graph(adj)\n",
    "    # adj + features\n",
    "    adj_label = adj_train + sp.eye(adj_train.shape[0])\n",
    "    # cerating array of tensor type\n",
    "    adj_label = torch.FloatTensor(adj_label.toarray())\n",
    "    # 468 * 468 - 2248 / 2248\n",
    "    # pos_weight= tensor([96.4306])\n",
    "    pos_weight = torch.Tensor([float(adj.shape[0] * adj.shape[0] - adj.sum()) / adj.sum()])\n",
    "    # 468 * 468 / ((468 * 468) - 2248) * 2\n",
    "    # norm= 0.5051850758386537\n",
    "    norm = adj.shape[0] * adj.shape[0] / float((adj.shape[0] * adj.shape[0] - adj.sum()) * 2)\n",
    "    # calling GCNModelVAE, passing shape, hidden1 hidden2, dropout\n",
    "    model = GCNModelVAE(feat_dim, args.hidden1, args.hidden2, args.dropout)\n",
    "    # print(\"model=\",model)\n",
    "    # predefined in torch\n",
    "    # Implements Adam algorithm\n",
    "    optimizer = optim.Adam(model.parameters(), lr=args.lr)\n",
    "\n",
    "    hidden_emb = None\n",
    "    for epoch in range(args.epochs):\n",
    "        # This method returns the time as a floating point number expressed in seconds\n",
    "        t = time.time()\n",
    "        # predefined in torch\n",
    "        model.train()\n",
    "        # predefined in torch\n",
    "        # Sets the gradients of all optimized torch.Tensor s to zero.\n",
    "        optimizer.zero_grad()\n",
    "        recovered, mu, logvar, z = model(features, adj_norm)\n",
    "        # print(\"shape=\",z.shape)\n",
    "        loss = loss_function(preds=recovered, labels=adj_label,\n",
    "                             mu=mu, logvar=logvar, n_nodes=n_nodes,\n",
    "                             norm=norm, pos_weight=pos_weight)\n",
    "        loss.backward()\n",
    "        cur_loss = loss.item()\n",
    "        optimizer.step()\n",
    "\n",
    "        hidden_emb = mu.data.numpy()\n",
    "        roc_curr, ap_curr = get_roc_score(hidden_emb, adj_orig, val_edges, val_edges_false)\n",
    "        # print(\"roc=\",roc_curr)\n",
    "    # print(\"Optimization Finished!\")\n",
    "    \n",
    "    roc_score, ap_score = get_roc_score(hidden_emb, adj_orig, test_edges, test_edges_false)\n",
    "    # print(\"Epoch:\", '%04d' % (epoch + 1), \"train_loss=\", \"{:.5f}\".format(cur_loss),\n",
    "    #     \"val_ap=\", \"{:.5f}\".format(ap_curr),\n",
    "    #     \"time=\", \"{:.5f}\".format(time.time() - t)\n",
    "    #     )\n",
    "    print(\"roc=\",roc_score)\n",
    "    print(\"epoch=\",args.epochs)\n",
    "    return(recovered)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc= 0.9910468319559229\n",
      "epoch= 200\n",
      "tensor([[ 3.6171,  0.1792,  2.2845,  ..., -0.7665, -0.3128, -0.5976],\n",
      "        [ 0.1792,  3.7346,  1.6494,  ...,  0.5010, -0.6115, -1.2976],\n",
      "        [ 2.2845,  1.6494,  4.0853,  ...,  0.3697, -0.4351, -0.7176],\n",
      "        ...,\n",
      "        [-0.7665,  0.5010,  0.3697,  ...,  3.1156, -0.4388, -0.7960],\n",
      "        [-0.3128, -0.6115, -0.4351,  ..., -0.4388,  2.6371,  2.5288],\n",
      "        [-0.5976, -1.2976, -0.7176,  ..., -0.7960,  2.5288,  4.3713]],\n",
      "       grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# z1=gae_for(\"./orl_dataset/person1/train_images/1_1.jpg\")\n",
    "# print(z1)\n",
    "# # z2=gae_for(\"./orl_dataset/person1/train_images/1_1.jpg\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "roc= 0.9943755739210285\n",
      "epoch= 200\n",
      "tensor([[ 4.1946,  1.4476,  3.3864,  ...,  0.5552, -0.6176, -1.2498],\n",
      "        [ 1.4476,  3.2265,  2.5017,  ...,  0.5765, -0.3381, -0.2570],\n",
      "        [ 3.3864,  2.5017,  4.8072,  ...,  0.0315, -0.2396, -0.2249],\n",
      "        ...,\n",
      "        [ 0.5552,  0.5765,  0.0315,  ...,  3.7132, -0.2838, -0.6492],\n",
      "        [-0.6176, -0.3381, -0.2396,  ..., -0.2838,  3.2180,  2.7918],\n",
      "        [-1.2498, -0.2570, -0.2249,  ..., -0.6492,  2.7918,  4.0866]],\n",
      "       grad_fn=<MmBackward>)\n"
     ]
    }
   ],
   "source": [
    "# z2=gae_for(\"./orl_dataset/person1/train_images/1_1.jpg\")\n",
    "# print(z2)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[0.5344736576080322]\n"
     ]
    }
   ],
   "source": [
    "# from scipy import spatial\n",
    "# z1=z1.detach().numpy()\n",
    "# z2=z2.detach().numpy()\n",
    "# from scipy import spatial\n",
    "# cosine_dist=[]\n",
    "# cosine_dist.append(1 - spatial.distance.cosine(np.array(z1).flatten() ,np.array(z2).flatten()))\n",
    "# max_cosine=cosine_dist.index(max(cosine_dist))\n",
    "# print(cosine_dist)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3.8.10 64-bit",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.10"
  },
  "orig_nbformat": 4,
  "vscode": {
   "interpreter": {
    "hash": "aadfbdf25e6c44ec54c3392f55309ee547c7867a1cc44e79bd77663d13a5dcc8"
   }
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
